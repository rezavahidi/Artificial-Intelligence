{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6baf2598",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-22T21:06:33.084910Z",
     "start_time": "2022-10-22T21:06:33.065246Z"
    }
   },
   "source": [
    "<br><font face=\"Times New Roman\" size=5><div dir=ltr align=center>\n",
    "<font color=blue size=8>\n",
    "    Introduction to Machine Learning <br>\n",
    "<font color=red size=5>\n",
    "    Sharif University of Technology - Computer Engineering Department <br>\n",
    "    Fall 2022<br> <br>\n",
    "<font color=black size=6>\n",
    "    Homework 2: Practical - Decision Tree   \n",
    "<font color=black size=4>\n",
    "    Hamidreza Yaghoubi \n",
    "    \n",
    "<br><br>\n",
    "<font size=4>\n",
    "In this homework, we are going to implement the Classification Decision Tree. Keep in mind to complete all of the following questions and write your own codes in the TODO cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3871b7",
   "metadata": {},
   "source": [
    "<font face=\"Times New Roman\" size=4><div dir=ltr>\n",
    "# Problem 2: Classification Decision Tree (100 points)\n",
    "We will implement a Classification Decision Tree from scratch in the following problem. Then we will use our model to predict malignant and benign breast cancer. For this purpose, we will use the breast_cancer.csv dataset which you can find more details about it <a href=\"https://www.kaggle.com/datasets/merishnasuwal/breast-cancer-prediction-dataset\"><font face=\"Roboto\">here</font></a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccdebb27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-22T21:30:24.876526Z",
     "start_time": "2022-10-22T21:30:24.439793Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from math import log\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2307c50",
   "metadata": {},
   "source": [
    "<font face=\"Times New Roman\" size=4><div dir=ltr>\n",
    "## Classification Decision Tree Class (60 points)\n",
    "In this section, you only need to fill TODO parts. You can find the logic and formula in both course slides and the web, but fill it out on your own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "618baebb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-22T21:30:25.034364Z",
     "start_time": "2022-10-22T21:30:25.021627Z"
    }
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "    def is_leaf(self):\n",
    "        if self.value is not None:\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71b3b483",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-22T21:30:25.439792Z",
     "start_time": "2022-10-22T21:30:25.404649Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "\n",
    "    def is_splitting_finished(self, depth, num_class_labels, num_samples):\n",
    "            \n",
    "        if depth == self.max_depth:\n",
    "            return True\n",
    "        elif num_class_labels == 1:\n",
    "            return True\n",
    "        elif num_samples <= self.min_samples_split:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        #TODO\n",
    "\n",
    "\n",
    "    def split(self, X, y, feature, threshold):\n",
    "        left_indexes = X[feature] <= threshold\n",
    "        right_indexes = -left_indexes\n",
    "        X_left = X[left_indexes]\n",
    "        y_left = y[left_indexes]\n",
    "        X_right = X[right_indexes]\n",
    "        y_right = y[right_indexes]\n",
    "\n",
    "        return X_left, X_right, y_left, y_right\n",
    "\n",
    "    def entropy(self, y):\n",
    "        \n",
    "        p_1 = len(y[y.diagnosis == 1]) / len(y)\n",
    "        ent = 0\n",
    "        if p_1 == 0 or p_1 == 1:\n",
    "            ent = 0\n",
    "        else:\n",
    "            ent = - (p_1 * log(p_1, 2) + (1 - p_1) * log(1 - p_1, 2))\n",
    "        \n",
    "        return ent\n",
    "\n",
    "\n",
    "    def information_gain(self, X, y, feature, threshold):\n",
    "        X_left, X_right, y_left, y_right = self.split(X, y, feature, threshold)\n",
    "        H_y = self.entropy(y)\n",
    "\n",
    "        e_right = 0\n",
    "        e_left = 0\n",
    "        if len(y_left) == 0 or len(y_right) == 0:\n",
    "            return 0\n",
    "        if len(y_left) != 0:\n",
    "            e_left = self.entropy(y_left)\n",
    "        if len(y_right) != 0:\n",
    "            e_right = self.entropy(y_right)\n",
    "        H_y_children = e_right * (len(y_right) / len(y)) + e_left * (len(y_left) / len(y))\n",
    "        \n",
    "        ig = H_y - H_y_children\n",
    "        \n",
    "        return ig\n",
    "\n",
    "    def best_split(self, X, y):\n",
    "        features = list(X.columns.values)\n",
    "        random.shuffle(features)\n",
    "        best_ig = 0\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        for f in features:\n",
    "            for t in list(set(list(X[f]))):\n",
    "                temp_ig = self.information_gain(X, y, f, t)\n",
    "                if temp_ig >= best_ig:\n",
    "                    best_ig = temp_ig\n",
    "                    best_feature = f\n",
    "                    best_threshold = t\n",
    "            \n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        if self.is_splitting_finished(depth, len(X.columns), len(X)):\n",
    "            return None\n",
    "\n",
    "        best_feature, best_threshold = self.best_split(X, y)\n",
    "        X_left, X_right, y_left, y_right = self.split(X, y, best_feature, best_threshold)\n",
    "\n",
    "        left_node = self.build_tree(X_left, y_left, depth=depth + 1)\n",
    "        right_node = self.build_tree(X_right, y_right, depth=depth + 1)\n",
    "\n",
    "        value = None\n",
    "        if left_node is None or right_node is None:\n",
    "            true_value = len(y[y['diagnosis'] == 1])\n",
    "            false_value = len(y[y['diagnosis'] == 0])\n",
    "            if true_value >= false_value:\n",
    "                value = 1\n",
    "            else:\n",
    "                value = 0\n",
    "\n",
    "        return Node(feature=best_feature, threshold=best_threshold, left=left_node, right=right_node, value=value)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self.build_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree = self.root\n",
    "        predicted_value = []\n",
    "        for index in list(X.index):\n",
    "            data = X.loc[index]\n",
    "            current_tree = tree\n",
    "            for depth in range(self.max_depth):\n",
    "                if Node.is_leaf(current_tree):\n",
    "                    predicted_value.append(current_tree.value)\n",
    "                    break\n",
    "                feature = current_tree.feature\n",
    "                threshold = current_tree.threshold\n",
    "                if data[feature] <= threshold:\n",
    "                    current_tree = current_tree.left\n",
    "                if data[feature] > threshold:\n",
    "                    current_tree = current_tree.right\n",
    "\n",
    "        return predicted_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2197f0",
   "metadata": {},
   "source": [
    "<font face=\"Times New Roman\" size=4><div dir=ltr>\n",
    "## Data Prepration (20 points)\n",
    "In this section, you must perform a good EDA for data. Then split it into train and validation data. We will then use the validation data to find the best model hyperparameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da5a4d94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-22T21:30:26.199958Z",
     "start_time": "2022-10-22T21:30:26.192910Z"
    }
   },
   "outputs": [],
   "source": [
    "breast_cancer_pdf = pd.read_csv(\"breast_cancer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27207399",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-22T21:30:28.478997Z",
     "start_time": "2022-10-22T21:30:28.476044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   mean_radius      569 non-null    float64\n",
      " 1   mean_texture     569 non-null    float64\n",
      " 2   mean_perimeter   569 non-null    float64\n",
      " 3   mean_area        569 non-null    float64\n",
      " 4   mean_smoothness  569 non-null    float64\n",
      " 5   diagnosis        569 non-null    int64  \n",
      "dtypes: float64(5), int64(1)\n",
      "memory usage: 26.8 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_radius</th>\n",
       "      <th>mean_texture</th>\n",
       "      <th>mean_perimeter</th>\n",
       "      <th>mean_area</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.627417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.483918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean_radius  mean_texture  mean_perimeter    mean_area  \\\n",
       "count   569.000000    569.000000      569.000000   569.000000   \n",
       "mean     14.127292     19.289649       91.969033   654.889104   \n",
       "std       3.524049      4.301036       24.298981   351.914129   \n",
       "min       6.981000      9.710000       43.790000   143.500000   \n",
       "25%      11.700000     16.170000       75.170000   420.300000   \n",
       "50%      13.370000     18.840000       86.240000   551.100000   \n",
       "75%      15.780000     21.800000      104.100000   782.700000   \n",
       "max      28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       mean_smoothness   diagnosis  \n",
       "count       569.000000  569.000000  \n",
       "mean          0.096360    0.627417  \n",
       "std           0.014064    0.483918  \n",
       "min           0.052630    0.000000  \n",
       "25%           0.086370    0.000000  \n",
       "50%           0.095870    1.000000  \n",
       "75%           0.105300    1.000000  \n",
       "max           0.163400    1.000000  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breast_cancer_pdf.info()\n",
    "breast_cancer_pdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39ba4c34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-22T21:30:38.003703Z",
     "start_time": "2022-10-22T21:30:37.996292Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X = breast_cancer_pdf.drop(['diagnosis'], axis=1)\n",
    "\n",
    "y = breast_cancer_pdf[['diagnosis']]\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.70, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefd7a92",
   "metadata": {},
   "source": [
    "<font face=\"Times New Roman\" size=4><div dir=ltr>\n",
    "## Training And Tuning Hyperparameters (20 points)\n",
    "In this section, you only need to find the best hyperparameters for your model. You can test different values and permutations of hyperparameters by adding them to the lists below. Your model must have at least accuracy=0.85 on validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "assigned-estate",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-22T21:36:15.733600Z",
     "start_time": "2022-10-22T21:36:15.730426Z"
    }
   },
   "outputs": [],
   "source": [
    "max_depths = []\n",
    "max_depths.extend(range(2, 10))\n",
    "min_samples_splits = []\n",
    "min_samples_splits.extend(range(2, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58003410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of training set for [min_samples_splits = 2 -max_depths = 2] = 0.8705882352941177\n",
      "accuracy of validation set for [min_samples_splits = 2 - max_depths = 2] = 0.8771929824561403\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 3 -max_depths = 2] = 0.8705882352941177\n",
      "accuracy of validation set for [min_samples_splits = 3 - max_depths = 2] = 0.8771929824561403\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 4 -max_depths = 2] = 0.8705882352941177\n",
      "accuracy of validation set for [min_samples_splits = 4 - max_depths = 2] = 0.8771929824561403\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 5 -max_depths = 2] = 0.8705882352941177\n",
      "accuracy of validation set for [min_samples_splits = 5 - max_depths = 2] = 0.8771929824561403\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 6 -max_depths = 2] = 0.8705882352941177\n",
      "accuracy of validation set for [min_samples_splits = 6 - max_depths = 2] = 0.8771929824561403\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 7 -max_depths = 2] = 0.8705882352941177\n",
      "accuracy of validation set for [min_samples_splits = 7 - max_depths = 2] = 0.8771929824561403\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 8 -max_depths = 2] = 0.8705882352941177\n",
      "accuracy of validation set for [min_samples_splits = 8 - max_depths = 2] = 0.8771929824561403\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 9 -max_depths = 2] = 0.8705882352941177\n",
      "accuracy of validation set for [min_samples_splits = 9 - max_depths = 2] = 0.8771929824561403\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 2 -max_depths = 3] = 0.8705882352941177\n",
      "accuracy of validation set for [min_samples_splits = 2 - max_depths = 3] = 0.8771929824561403\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 3 -max_depths = 3] = 0.8705882352941177\n",
      "accuracy of validation set for [min_samples_splits = 3 - max_depths = 3] = 0.8771929824561403\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 4 -max_depths = 3] = 0.8705882352941177\n",
      "accuracy of validation set for [min_samples_splits = 4 - max_depths = 3] = 0.8771929824561403\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 5 -max_depths = 3] = 0.8705882352941177\n",
      "accuracy of validation set for [min_samples_splits = 5 - max_depths = 3] = 0.8771929824561403\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 6 -max_depths = 3] = 0.8705882352941177\n",
      "accuracy of validation set for [min_samples_splits = 6 - max_depths = 3] = 0.8771929824561403\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 7 -max_depths = 3] = 0.8705882352941177\n",
      "accuracy of validation set for [min_samples_splits = 7 - max_depths = 3] = 0.8771929824561403\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 8 -max_depths = 3] = 0.8705882352941177\n",
      "accuracy of validation set for [min_samples_splits = 8 - max_depths = 3] = 0.8771929824561403\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 9 -max_depths = 3] = 0.8705882352941177\n",
      "accuracy of validation set for [min_samples_splits = 9 - max_depths = 3] = 0.8771929824561403\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 2 -max_depths = 4] = 0.9176470588235294\n",
      "accuracy of validation set for [min_samples_splits = 2 - max_depths = 4] = 0.8721804511278195\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 3 -max_depths = 4] = 0.9176470588235294\n",
      "accuracy of validation set for [min_samples_splits = 3 - max_depths = 4] = 0.8721804511278195\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 4 -max_depths = 4] = 0.9176470588235294\n",
      "accuracy of validation set for [min_samples_splits = 4 - max_depths = 4] = 0.8721804511278195\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 5 -max_depths = 4] = 0.888235294117647\n",
      "accuracy of validation set for [min_samples_splits = 5 - max_depths = 4] = 0.8421052631578947\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 6 -max_depths = 4] = 0.888235294117647\n",
      "accuracy of validation set for [min_samples_splits = 6 - max_depths = 4] = 0.8421052631578947\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 7 -max_depths = 4] = 0.888235294117647\n",
      "accuracy of validation set for [min_samples_splits = 7 - max_depths = 4] = 0.8421052631578947\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 8 -max_depths = 4] = 0.888235294117647\n",
      "accuracy of validation set for [min_samples_splits = 8 - max_depths = 4] = 0.8421052631578947\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 9 -max_depths = 4] = 0.888235294117647\n",
      "accuracy of validation set for [min_samples_splits = 9 - max_depths = 4] = 0.8421052631578947\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 2 -max_depths = 5] = 0.9294117647058824\n",
      "accuracy of validation set for [min_samples_splits = 2 - max_depths = 5] = 0.9047619047619048\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 3 -max_depths = 5] = 0.9294117647058824\n",
      "accuracy of validation set for [min_samples_splits = 3 - max_depths = 5] = 0.9047619047619048\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 4 -max_depths = 5] = 0.9294117647058824\n",
      "accuracy of validation set for [min_samples_splits = 4 - max_depths = 5] = 0.9047619047619048\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 5 -max_depths = 5] = 0.888235294117647\n",
      "accuracy of validation set for [min_samples_splits = 5 - max_depths = 5] = 0.8421052631578947\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 6 -max_depths = 5] = 0.888235294117647\n",
      "accuracy of validation set for [min_samples_splits = 6 - max_depths = 5] = 0.8421052631578947\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 7 -max_depths = 5] = 0.888235294117647\n",
      "accuracy of validation set for [min_samples_splits = 7 - max_depths = 5] = 0.8421052631578947\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 8 -max_depths = 5] = 0.888235294117647\n",
      "accuracy of validation set for [min_samples_splits = 8 - max_depths = 5] = 0.8421052631578947\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 9 -max_depths = 5] = 0.888235294117647\n",
      "accuracy of validation set for [min_samples_splits = 9 - max_depths = 5] = 0.8421052631578947\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 2 -max_depths = 6] = 0.9470588235294117\n",
      "accuracy of validation set for [min_samples_splits = 2 - max_depths = 6] = 0.9223057644110275\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 3 -max_depths = 6] = 0.9294117647058824\n",
      "accuracy of validation set for [min_samples_splits = 3 - max_depths = 6] = 0.9047619047619048\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 4 -max_depths = 6] = 0.9294117647058824\n",
      "accuracy of validation set for [min_samples_splits = 4 - max_depths = 6] = 0.9047619047619048\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 5 -max_depths = 6] = 0.888235294117647\n",
      "accuracy of validation set for [min_samples_splits = 5 - max_depths = 6] = 0.8421052631578947\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of training set for [min_samples_splits = 6 -max_depths = 6] = 0.888235294117647\n",
      "accuracy of validation set for [min_samples_splits = 6 - max_depths = 6] = 0.8421052631578947\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 7 -max_depths = 6] = 0.888235294117647\n",
      "accuracy of validation set for [min_samples_splits = 7 - max_depths = 6] = 0.8421052631578947\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 8 -max_depths = 6] = 0.888235294117647\n",
      "accuracy of validation set for [min_samples_splits = 8 - max_depths = 6] = 0.8421052631578947\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 9 -max_depths = 6] = 0.888235294117647\n",
      "accuracy of validation set for [min_samples_splits = 9 - max_depths = 6] = 0.8421052631578947\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 2 -max_depths = 7] = 0.9529411764705882\n",
      "accuracy of validation set for [min_samples_splits = 2 - max_depths = 7] = 0.9047619047619048\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 3 -max_depths = 7] = 0.9294117647058824\n",
      "accuracy of validation set for [min_samples_splits = 3 - max_depths = 7] = 0.9047619047619048\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 4 -max_depths = 7] = 0.9294117647058824\n",
      "accuracy of validation set for [min_samples_splits = 4 - max_depths = 7] = 0.9047619047619048\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 5 -max_depths = 7] = 0.888235294117647\n",
      "accuracy of validation set for [min_samples_splits = 5 - max_depths = 7] = 0.8421052631578947\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 6 -max_depths = 7] = 0.888235294117647\n",
      "accuracy of validation set for [min_samples_splits = 6 - max_depths = 7] = 0.8421052631578947\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 7 -max_depths = 7] = 0.888235294117647\n",
      "accuracy of validation set for [min_samples_splits = 7 - max_depths = 7] = 0.8421052631578947\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 8 -max_depths = 7] = 0.888235294117647\n",
      "accuracy of validation set for [min_samples_splits = 8 - max_depths = 7] = 0.8421052631578947\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 9 -max_depths = 7] = 0.888235294117647\n",
      "accuracy of validation set for [min_samples_splits = 9 - max_depths = 7] = 0.8421052631578947\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 2 -max_depths = 8] = 0.9529411764705882\n",
      "accuracy of validation set for [min_samples_splits = 2 - max_depths = 8] = 0.9047619047619048\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 3 -max_depths = 8] = 0.9294117647058824\n",
      "accuracy of validation set for [min_samples_splits = 3 - max_depths = 8] = 0.9047619047619048\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 4 -max_depths = 8] = 0.9294117647058824\n",
      "accuracy of validation set for [min_samples_splits = 4 - max_depths = 8] = 0.9047619047619048\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 5 -max_depths = 8] = 0.888235294117647\n",
      "accuracy of validation set for [min_samples_splits = 5 - max_depths = 8] = 0.8421052631578947\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 6 -max_depths = 8] = 0.888235294117647\n",
      "accuracy of validation set for [min_samples_splits = 6 - max_depths = 8] = 0.8421052631578947\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 7 -max_depths = 8] = 0.888235294117647\n",
      "accuracy of validation set for [min_samples_splits = 7 - max_depths = 8] = 0.8421052631578947\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 8 -max_depths = 8] = 0.888235294117647\n",
      "accuracy of validation set for [min_samples_splits = 8 - max_depths = 8] = 0.8421052631578947\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 9 -max_depths = 8] = 0.888235294117647\n",
      "accuracy of validation set for [min_samples_splits = 9 - max_depths = 8] = 0.8421052631578947\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 2 -max_depths = 9] = 0.9529411764705882\n",
      "accuracy of validation set for [min_samples_splits = 2 - max_depths = 9] = 0.9047619047619048\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 3 -max_depths = 9] = 0.9294117647058824\n",
      "accuracy of validation set for [min_samples_splits = 3 - max_depths = 9] = 0.9047619047619048\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 4 -max_depths = 9] = 0.9294117647058824\n",
      "accuracy of validation set for [min_samples_splits = 4 - max_depths = 9] = 0.9047619047619048\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 5 -max_depths = 9] = 0.888235294117647\n",
      "accuracy of validation set for [min_samples_splits = 5 - max_depths = 9] = 0.8421052631578947\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 6 -max_depths = 9] = 0.888235294117647\n",
      "accuracy of validation set for [min_samples_splits = 6 - max_depths = 9] = 0.8421052631578947\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 7 -max_depths = 9] = 0.888235294117647\n",
      "accuracy of validation set for [min_samples_splits = 7 - max_depths = 9] = 0.8421052631578947\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 8 -max_depths = 9] = 0.888235294117647\n",
      "accuracy of validation set for [min_samples_splits = 8 - max_depths = 9] = 0.8421052631578947\n",
      "------------------------------------------------\n",
      "accuracy of training set for [min_samples_splits = 9 -max_depths = 9] = 0.888235294117647\n",
      "accuracy of validation set for [min_samples_splits = 9 - max_depths = 9] = 0.8421052631578947\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_max_depth = None\n",
    "best_min_samples_split = None\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "for max_depth in max_depths:\n",
    "    for min_samples_split in min_samples_splits:\n",
    "        clf = DecisionTree(max_depth, min_samples_split)\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_val_pred = clf.predict(x_val)\n",
    "        y_train_pred = clf.predict(x_train)\n",
    "        accuracy = accuracy_score(y_val_pred, y_val)\n",
    "        train_accuracy = accuracy_score(y_train_pred, y_train)\n",
    "        print(f\"accuracy of training set for [min_samples_splits = {min_samples_split} -max_depths = {max_depth}] = {train_accuracy}\")\n",
    "        print(f\"accuracy of validation set for [min_samples_splits = {min_samples_split} - max_depths = {max_depth}] = {accuracy}\")\n",
    "        print(\"------------------------------------------------\")\n",
    "        if accuracy >= best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_max_depth = max_depth\n",
    "            best_min_samples_split = min_samples_split\n",
    "            best_model = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c008df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
